{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LSTM, Bidirectional, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                           roc_curve, precision_recall_curve, average_precision_score,\n",
    "                           accuracy_score, precision_score, recall_score, f1_score)\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "def load_peptide_data(data_path, max_sequences_per_class=20000):\n",
    "    \"\"\"\n",
    "    Load peptide sequences from CSV files with comprehensive error handling and statistics\n",
    "    Optimized for binary classification with balanced dataset creation\n",
    "    \"\"\"\n",
    "    therapeutic_path = os.path.join(data_path, \"Therapeutic data\")\n",
    "    non_therapeutic_path = os.path.join(data_path, \"Non-Therapeutic data\")\n",
    "    \n",
    "    sequences, labels = [], []\n",
    "    file_stats = {}\n",
    "    \n",
    "    if not os.path.exists(therapeutic_path):\n",
    "        print(f\"Warning: {therapeutic_path} not found!\")\n",
    "        return [], [], {}\n",
    "    \n",
    "    if not os.path.exists(non_therapeutic_path):\n",
    "        print(f\"Warning: {non_therapeutic_path} not found!\")\n",
    "        return [], [], {}\n",
    "    \n",
    "    print(\"Loading therapeutic peptides for binary classification...\")\n",
    "    therapeutic_total = 0\n",
    "    therapeutic_files = [f for f in os.listdir(therapeutic_path) if f.endswith('.csv')]\n",
    "    \n",
    "    for filename in therapeutic_files:\n",
    "        file_path = os.path.join(therapeutic_path, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if df.shape[1] >= 1:\n",
    "                seqs = df.iloc[:, 0].dropna().astype(str).tolist()\n",
    "                seqs = [seq.strip().upper() for seq in seqs if seq.strip()]\n",
    "                \n",
    "                # Validate and clean sequences\n",
    "                valid_seqs = []\n",
    "                for seq in seqs:\n",
    "                    clean_seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "                    if 5 <= len(clean_seq) <= 200:\n",
    "                        valid_seqs.append(clean_seq)\n",
    "                \n",
    "                # Limit sequences to prevent memory issues and ensure balance\n",
    "                if max_sequences_per_class and therapeutic_total + len(valid_seqs) > max_sequences_per_class:\n",
    "                    remaining = max(0, max_sequences_per_class - therapeutic_total)\n",
    "                    valid_seqs = valid_seqs[:remaining]\n",
    "                \n",
    "                sequences.extend(valid_seqs)\n",
    "                labels.extend([1] * len(valid_seqs))\n",
    "                therapeutic_total += len(valid_seqs)\n",
    "                file_stats[f\"therapeutic_{filename}\"] = len(valid_seqs)\n",
    "                print(f\"  ✓ {filename}: {len(valid_seqs)} valid sequences\")\n",
    "                \n",
    "                if max_sequences_per_class and therapeutic_total >= max_sequences_per_class:\n",
    "                    break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error reading {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Total therapeutic sequences: {therapeutic_total}\")\n",
    "    \n",
    "    print(\"\\nLoading non-therapeutic peptides for binary classification...\")\n",
    "    non_therapeutic_total = 0\n",
    "    non_therapeutic_files = [f for f in os.listdir(non_therapeutic_path) if f.endswith('.csv')]\n",
    "    \n",
    "    for filename in non_therapeutic_files:\n",
    "        file_path = os.path.join(non_therapeutic_path, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if df.shape[1] >= 1:\n",
    "                seqs = df.iloc[:, 0].dropna().astype(str).tolist()\n",
    "                seqs = [seq.strip().upper() for seq in seqs if seq.strip()]\n",
    "                \n",
    "                # Validate and clean sequences\n",
    "                valid_seqs = []\n",
    "                for seq in seqs:\n",
    "                    clean_seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "                    if 5 <= len(clean_seq) <= 200:\n",
    "                        valid_seqs.append(clean_seq)\n",
    "                \n",
    "                # Limit sequences to maintain balance with therapeutic sequences\n",
    "                if max_sequences_per_class and non_therapeutic_total + len(valid_seqs) > therapeutic_total:\n",
    "                    remaining = max(0, therapeutic_total - non_therapeutic_total)\n",
    "                    valid_seqs = valid_seqs[:remaining]\n",
    "                \n",
    "                sequences.extend(valid_seqs)\n",
    "                labels.extend([0] * len(valid_seqs))\n",
    "                non_therapeutic_total += len(valid_seqs)\n",
    "                file_stats[f\"non_therapeutic_{filename}\"] = len(valid_seqs)\n",
    "                print(f\"  ✓ {filename}: {len(valid_seqs)} valid sequences\")\n",
    "                \n",
    "                if non_therapeutic_total >= therapeutic_total:\n",
    "                    break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error reading {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Total non-therapeutic sequences: {non_therapeutic_total}\")\n",
    "    print(f\"Overall total: {len(sequences)} sequences\")\n",
    "    print(f\"Dataset balance: {sum(labels)/len(labels)*100:.1f}% therapeutic\")\n",
    "    \n",
    "    return sequences, labels, file_stats\n",
    "\n",
    "def create_advanced_binary_classifier(input_dim):\n",
    "    \"\"\"\n",
    "    Create advanced deep neural network optimized for binary classification\n",
    "    with enhanced architecture and regularization\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(input_dim,), name='feature_input')\n",
    "    \n",
    "    # First dense block\n",
    "    x = Dense(512, activation='relu', name='dense_1')(input_layer)\n",
    "    x = BatchNormalization(name='bn_1')(x)\n",
    "    x = Dropout(0.4, name='dropout_1')(x)\n",
    "    \n",
    "    # Second dense block\n",
    "    x = Dense(256, activation='relu', name='dense_2')(x)\n",
    "    x = BatchNormalization(name='bn_2')(x)\n",
    "    x = Dropout(0.3, name='dropout_2')(x)\n",
    "    \n",
    "    # Third dense block\n",
    "    x = Dense(128, activation='relu', name='dense_3')(x)\n",
    "    x = BatchNormalization(name='bn_3')(x)\n",
    "    x = Dropout(0.3, name='dropout_3')(x)\n",
    "    \n",
    "    # Fourth dense block\n",
    "    x = Dense(64, activation='relu', name='dense_4')(x)\n",
    "    x = BatchNormalization(name='bn_4')(x)\n",
    "    x = Dropout(0.2, name='dropout_4')(x)\n",
    "    \n",
    "    # Final classification layer\n",
    "    output = Dense(1, activation='sigmoid', name='therapeutic_prediction')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output, name='BinaryTherapeuticClassifier')\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', Precision(name='precision'), Recall(name='recall'), AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_ensemble_voting_classifier(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Create ensemble voting classifier combining multiple algorithms\n",
    "    \"\"\"\n",
    "    # Base classifiers\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=15, \n",
    "        min_samples_split=5,\n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    gb_classifier = GradientBoostingClassifier(\n",
    "        n_estimators=150,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    svc_classifier = CalibratedClassifierCV(\n",
    "        SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42),\n",
    "        cv=3\n",
    "    )\n",
    "    \n",
    "    lr_classifier = LogisticRegression(\n",
    "        C=1.0, \n",
    "        max_iter=1000, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create voting ensemble\n",
    "    voting_classifier = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('random_forest', rf_classifier),\n",
    "            ('gradient_boosting', gb_classifier),\n",
    "            ('svm', svc_classifier),\n",
    "            ('logistic_regression', lr_classifier)\n",
    "        ],\n",
    "        voting='soft'  # Use predicted probabilities\n",
    "    )\n",
    "    \n",
    "    return voting_classifier\n",
    "\n",
    "def extract_comprehensive_binary_features(sequences):\n",
    "    \"\"\"\n",
    "    Extract optimized features specifically for binary therapeutic classification\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    print(\"Extracting binary classification optimized features...\")\n",
    "    \n",
    "    for seq in tqdm(sequences, desc=\"Processing sequences\"):\n",
    "        seq_len = len(seq)\n",
    "        feature_vector = []\n",
    "        \n",
    "        # 1. Basic sequence properties\n",
    "        feature_vector.append(seq_len)\n",
    "        \n",
    "        # 2. Amino acid composition (20 features)\n",
    "        aa_composition = {}\n",
    "        for aa in 'ACDEFGHIKLMNPQRSTVWY':\n",
    "            aa_composition[aa] = seq.count(aa) / seq_len\n",
    "            feature_vector.append(aa_composition[aa])\n",
    "        \n",
    "        # 3. Key therapeutic indicators\n",
    "        # Cationic amino acids (important for antimicrobial activity)\n",
    "        cationic = (seq.count('K') + seq.count('R') + seq.count('H')) / seq_len\n",
    "        \n",
    "        # Hydrophobic amino acids\n",
    "        hydrophobic = sum(seq.count(aa) for aa in 'AILMFWV') / seq_len\n",
    "        \n",
    "        # Aromatic amino acids\n",
    "        aromatic = sum(seq.count(aa) for aa in 'FWY') / seq_len\n",
    "        \n",
    "        # Amphipathic properties\n",
    "        positive_charge = seq.count('K') + seq.count('R') + seq.count('H')\n",
    "        negative_charge = seq.count('D') + seq.count('E')\n",
    "        net_charge = positive_charge - negative_charge\n",
    "        \n",
    "        feature_vector.extend([\n",
    "            cationic, hydrophobic, aromatic,\n",
    "            positive_charge / seq_len,\n",
    "            negative_charge / seq_len,\n",
    "            net_charge / seq_len,\n",
    "            abs(net_charge) / seq_len\n",
    "        ])\n",
    "        \n",
    "        # 4. Therapeutic peptide motifs\n",
    "        # Common therapeutic dipeptides\n",
    "        therapeutic_dipeptides = ['KL', 'LK', 'RW', 'WR', 'FK', 'KF', 'LL', 'KK', 'RR']\n",
    "        for dp in therapeutic_dipeptides:\n",
    "            count = 0\n",
    "            for i in range(len(seq) - 1):\n",
    "                if seq[i:i+2] == dp:\n",
    "                    count += 1\n",
    "            feature_vector.append(count / (seq_len - 1) if seq_len > 1 else 0)\n",
    "        \n",
    "        # 5. Structural features\n",
    "        proline_content = seq.count('P') / seq_len\n",
    "        glycine_content = seq.count('G') / seq_len\n",
    "        cysteine_content = seq.count('C') / seq_len\n",
    "        \n",
    "        feature_vector.extend([proline_content, glycine_content, cysteine_content])\n",
    "        \n",
    "        # 6. Hydrophobic moment (therapeutic indicator)\n",
    "        hydrophobic_values = {\n",
    "            'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,\n",
    "            'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,\n",
    "            'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,\n",
    "            'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2\n",
    "        }\n",
    "        \n",
    "        hydrophobic_moment = 0\n",
    "        for i, aa in enumerate(seq):\n",
    "            if aa in hydrophobic_values:\n",
    "                angle = i * 100 * np.pi / 180\n",
    "                hydrophobic_moment += hydrophobic_values[aa] * np.exp(1j * angle)\n",
    "        \n",
    "        feature_vector.append(abs(hydrophobic_moment) / seq_len)\n",
    "        \n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    features_array = np.array(features, dtype=np.float32)\n",
    "    print(f\"Binary classification features extracted: {features_array.shape[1]} features per sequence\")\n",
    "    \n",
    "    return features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ADVANCED BINARY CLASSIFICATION FOR THERAPEUTIC PEPTIDES ===\\n\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(\"../backend/models\", exist_ok=True)\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# Load data with correct relative paths\n",
    "data_path = \"../data\"\n",
    "sequences, labels, file_stats = load_peptide_data(data_path, max_sequences_per_class=15000)\n",
    "\n",
    "# Display dataset statistics\n",
    "if file_stats:\n",
    "    print(\"\\n📊 Dataset Statistics by File:\")\n",
    "    for file_key, count in file_stats.items():\n",
    "        print(f\"  {file_key}: {count:,} sequences\")\n",
    "\n",
    "print(f\"\\nFinal binary classification dataset:\")\n",
    "print(f\"Total sequences: {len(sequences):,}\")\n",
    "print(f\"Therapeutic peptides: {sum(labels):,} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "print(f\"Non-therapeutic peptides: {len(labels) - sum(labels):,} ({(len(labels) - sum(labels))/len(labels)*100:.1f}%)\")\n",
    "\n",
    "if len(sequences) == 0:\n",
    "    print(\"ERROR: No valid sequences found!\")\n",
    "    print(\"Please check your data files in ../data/Therapeutic data/ and ../data/Non-Therapeutic data/\")\n",
    "    exit()\n",
    "\n",
    "# Feature extraction strategy\n",
    "use_protbert = True\n",
    "protbert_file = \"../data/processed/protbert_embeddings.npy\"\n",
    "custom_features_file = \"../data/processed/binary_custom_features.npy\"\n",
    "\n",
    "if os.path.exists(protbert_file):\n",
    "    print(\"\\n🧬 Loading cached ProtBERT embeddings...\")\n",
    "    try:\n",
    "        protbert_features = np.load(protbert_file)\n",
    "        if len(protbert_features) == len(sequences):\n",
    "            print(\"✅ ProtBERT embeddings loaded successfully\")\n",
    "            features = protbert_features\n",
    "        else:\n",
    "            print(\"⚠️ ProtBERT embeddings size mismatch, extracting custom features...\")\n",
    "            use_protbert = False\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading ProtBERT embeddings: {e}\")\n",
    "        use_protbert = False\n",
    "else:\n",
    "    print(\"⚠️ ProtBERT embeddings not found, extracting custom features...\")\n",
    "    use_protbert = False\n",
    "\n",
    "if not use_protbert:\n",
    "    # Extract custom features optimized for binary classification\n",
    "    if os.path.exists(custom_features_file):\n",
    "        print(\"📁 Loading cached custom binary features...\")\n",
    "        features = np.load(custom_features_file)\n",
    "    else:\n",
    "        print(\"🔬 Extracting custom binary classification features...\")\n",
    "        features = extract_comprehensive_binary_features(sequences)\n",
    "        np.save(custom_features_file, features)\n",
    "        print(\"💾 Custom features cached for future use\")\n",
    "\n",
    "print(f\"Feature matrix shape: {features.shape}\")\n",
    "\n",
    "# Feature scaling\n",
    "print(\"\\n⚖️ Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Save scaler\n",
    "with open(\"../backend/models/binary_classification_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"✅ Scaler saved to ../backend/models/binary_classification_scaler.pkl\")\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_scaled, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Data Split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"  Features: {features_scaled.shape[1]}\")\n",
    "print(f\"  Training therapeutic ratio: {sum(y_train)/len(y_train)*100:.1f}%\")\n",
    "print(f\"  Test therapeutic ratio: {sum(y_test)/len(y_test)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train advanced neural network model\n",
    "print(\"\\n🏗️ Building Advanced Binary Classification Model...\")\n",
    "nn_model = create_advanced_binary_classifier(features_scaled.shape[1])\n",
    "nn_model.summary()\n",
    "\n",
    "# Enhanced callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=20, \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../backend/models/advanced_binary_classifier.h5', \n",
    "        save_best_only=True, \n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5, \n",
    "        patience=10, \n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\n🚀 Training Neural Network...\")\n",
    "history = nn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate neural network\n",
    "print(\"\\n📊 Evaluating Neural Network...\")\n",
    "nn_results = nn_model.evaluate(X_test, y_test, verbose=0)\n",
    "nn_loss, nn_acc, nn_precision, nn_recall, nn_auc = nn_results\n",
    "\n",
    "y_pred_proba_nn = nn_model.predict(X_test, verbose=0)\n",
    "y_pred_nn = (y_pred_proba_nn > 0.5).astype(int).flatten()\n",
    "\n",
    "nn_f1 = f1_score(y_test, y_pred_nn)\n",
    "nn_ap = average_precision_score(y_test, y_pred_proba_nn)\n",
    "\n",
    "print(f\"Neural Network Results:\")\n",
    "print(f\"  Accuracy: {nn_acc:.4f}\")\n",
    "print(f\"  Precision: {nn_precision:.4f}\")\n",
    "print(f\"  Recall: {nn_recall:.4f}\")\n",
    "print(f\"  F1-Score: {nn_f1:.4f}\")\n",
    "print(f\"  AUC-ROC: {nn_auc:.4f}\")\n",
    "print(f\"  AUC-PR: {nn_ap:.4f}\")\n",
    "\n",
    "# Train ensemble voting classifier\n",
    "print(\"\\n🤝 Training Ensemble Voting Classifier...\")\n",
    "ensemble_model = create_ensemble_voting_classifier(X_train, y_train)\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate ensemble\n",
    "y_pred_proba_ensemble = ensemble_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "\n",
    "ensemble_acc = accuracy_score(y_test, y_pred_ensemble)\n",
    "ensemble_precision = precision_score(y_test, y_pred_ensemble)\n",
    "ensemble_recall = recall_score(y_test, y_pred_ensemble)\n",
    "ensemble_f1 = f1_score(y_test, y_pred_ensemble)\n",
    "ensemble_auc = roc_auc_score(y_test, y_pred_proba_ensemble)\n",
    "ensemble_ap = average_precision_score(y_test, y_pred_proba_ensemble)\n",
    "\n",
    "print(f\"Ensemble Results:\")\n",
    "print(f\"  Accuracy: {ensemble_acc:.4f}\")\n",
    "print(f\"  Precision: {ensemble_precision:.4f}\")\n",
    "print(f\"  Recall: {ensemble_recall:.4f}\")\n",
    "print(f\"  F1-Score: {ensemble_f1:.4f}\")\n",
    "print(f\"  AUC-ROC: {ensemble_auc:.4f}\")\n",
    "print(f\"  AUC-PR: {ensemble_ap:.4f}\")\n",
    "\n",
    "# Save ensemble model\n",
    "with open(\"../backend/models/ensemble_binary_classifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ensemble_model, f)\n",
    "\n",
    "# Cross-validation for robust evaluation\n",
    "print(\"\\n🔄 Performing 5-Fold Cross-Validation on Ensemble...\")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(ensemble_model, features_scaled, labels, cv=skf, scoring='f1')\n",
    "print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "print(f\"Mean F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Detailed classification reports\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEURAL NETWORK CLASSIFICATION REPORT:\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_pred_nn, target_names=['Non-Therapeutic', 'Therapeutic']))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE CLASSIFICATION REPORT:\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_pred_ensemble, target_names=['Non-Therapeutic', 'Therapeutic']))\n",
    "\n",
    "# Enhanced visualization\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Training History - Accuracy\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Training', linewidth=2, color='blue')\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Validation', linewidth=2, color='red')\n",
    "axes[0, 0].set_title('Neural Network - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Training History - Loss\n",
    "axes[0, 1].plot(history.history['loss'], label='Training', linewidth=2, color='blue')\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Validation', linewidth=2, color='red')\n",
    "axes[0, 1].set_title('Neural Network - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. ROC Curves Comparison\n",
    "fpr_nn, tpr_nn, _ = roc_curve(y_test, y_pred_proba_nn)\n",
    "fpr_ensemble, tpr_ensemble, _ = roc_curve(y_test, y_pred_proba_ensemble)\n",
    "\n",
    "axes[0, 2].plot(fpr_nn, tpr_nn, label=f'Neural Network (AUC = {nn_auc:.3f})', linewidth=2)\n",
    "axes[0, 2].plot(fpr_ensemble, tpr_ensemble, label=f'Ensemble (AUC = {ensemble_auc:.3f})', linewidth=2)\n",
    "axes[0, 2].plot([0, 1], [0, 1], 'k--', label='Random Classifier', alpha=0.5)\n",
    "axes[0, 2].set_title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('False Positive Rate')\n",
    "axes[0, 2].set_ylabel('True Positive Rate')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Precision-Recall Curves\n",
    "precision_nn, recall_nn, _ = precision_recall_curve(y_test, y_pred_proba_nn)\n",
    "precision_ensemble, recall_ensemble, _ = precision_recall_curve(y_test, y_pred_proba_ensemble)\n",
    "\n",
    "axes[1, 0].plot(recall_nn, precision_nn, label=f'Neural Network (AP = {nn_ap:.3f})', linewidth=2)\n",
    "axes[1, 0].plot(recall_ensemble, precision_ensemble, label=f'Ensemble (AP = {ensemble_ap:.3f})', linewidth=2)\n",
    "axes[1, 0].axhline(y=sum(y_test)/len(y_test), color='k', linestyle='--', alpha=0.5, label='Baseline')\n",
    "axes[1, 0].set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Recall')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Model Performance Comparison\n",
    "models = ['Neural Network', 'Ensemble']\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "nn_values = [nn_acc, nn_precision, nn_recall, nn_f1, nn_auc]\n",
    "ensemble_values = [ensemble_acc, ensemble_precision, ensemble_recall, ensemble_f1, ensemble_auc]\n",
    "\n",
    "x_pos = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 1].bar(x_pos - width/2, nn_values, width, label='Neural Network', alpha=0.8, color='blue')\n",
    "axes[1, 1].bar(x_pos + width/2, ensemble_values, width, label='Ensemble', alpha=0.8, color='orange')\n",
    "axes[1, 1].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Metrics')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(metrics, rotation=45)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "# 6. Confusion Matrices\n",
    "cm_nn = confusion_matrix(y_test, y_pred_nn)\n",
    "cm_ensemble = confusion_matrix(y_test, y_pred_ensemble)\n",
    "\n",
    "# Neural Network Confusion Matrix\n",
    "im1 = axes[1, 2].imshow(cm_nn, interpolation='nearest', cmap='Blues')\n",
    "axes[1, 2].set_title('Neural Network - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "tick_marks = np.arange(2)\n",
    "axes[1, 2].set_xticks(tick_marks)\n",
    "axes[1, 2].set_yticks(tick_marks)\n",
    "axes[1, 2].set_xticklabels(['Non-Therapeutic', 'Therapeutic'])\n",
    "axes[1, 2].set_yticklabels(['Non-Therapeutic', 'Therapeutic'])\n",
    "axes[1, 2].set_ylabel('True Label')\n",
    "axes[1, 2].set_xlabel('Predicted Label')\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm_nn.max() / 2\n",
    "for i, j in np.ndindex(cm_nn.shape):\n",
    "    axes[1, 2].text(j, i, format(cm_nn[i, j], 'd'),\n",
    "                   ha=\"center\", va=\"center\",\n",
    "                   color=\"white\" if cm_nn[i, j] > thresh else \"black\",\n",
    "                   fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Separate figure for ensemble confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_ensemble, annot=True, fmt='d', cmap='Oranges', \n",
    "            xticklabels=['Non-Therapeutic', 'Therapeutic'],\n",
    "            yticklabels=['Non-Therapeutic', 'Therapeutic'])\n",
    "plt.title('Ensemble Model - Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"🏆 BINARY CLASSIFICATION PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Model':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'AUC-ROC':<10} {'AUC-PR':<10}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'Neural Network':<20} {nn_acc:<10.4f} {nn_precision:<10.4f} {nn_recall:<10.4f} {nn_f1:<10.4f} {nn_auc:<10.4f} {nn_ap:<10.4f}\")\n",
    "print(f\"{'Ensemble':<20} {ensemble_acc:<10.4f} {ensemble_precision:<10.4f} {ensemble_recall:<10.4f} {ensemble_f1:<10.4f} {ensemble_auc:<10.4f} {ensemble_ap:<10.4f}\")\n",
    "\n",
    "# Determine best model\n",
    "if ensemble_f1 > nn_f1:\n",
    "    best_model = \"Ensemble\"\n",
    "    best_f1 = ensemble_f1\n",
    "else:\n",
    "    best_model = \"Neural Network\"\n",
    "    best_f1 = nn_f1\n",
    "\n",
    "print(f\"\\n🥇 Best performing model: {best_model} (F1-Score: {best_f1:.4f})\")\n",
    "\n",
    "print(\"\\n✅ Binary classification completed!\")\n",
    "print(\"📁 Models saved:\")\n",
    "print(\"  - Neural Network: ../backend/models/advanced_binary_classifier.h5\")\n",
    "print(\"  - Ensemble: ../backend/models/ensemble_binary_classifier.pkl\")\n",
    "print(\"  - Scaler: ../backend/models/binary_classification_scaler.pkl\")\n",
    "print(f\"🔬 Feature extraction method: {'ProtBERT embeddings' if use_protbert else 'Custom biochemical features'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
