{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import lru_cache\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    for device in gpu_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eb74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "protbert_model = AutoModel.from_pretrained(\"Rostlab/prot_bert\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_peptide_data(data_path, max_sequences=50000):\n",
    "    \"\"\"\n",
    "    Load peptide sequences from CSV files with comprehensive error handling and statistics\n",
    "    \"\"\"\n",
    "    therapeutic_path = os.path.join(data_path, \"Therapeutic data\")\n",
    "    non_therapeutic_path = os.path.join(data_path, \"Non-Therapeutic data\")\n",
    "    \n",
    "    sequences, labels = [], []\n",
    "    file_stats = {}\n",
    "    \n",
    "    if not os.path.exists(therapeutic_path):\n",
    "        print(f\"Warning: {therapeutic_path} not found!\")\n",
    "        return [], [], {}\n",
    "    \n",
    "    if not os.path.exists(non_therapeutic_path):\n",
    "        print(f\"Warning: {non_therapeutic_path} not found!\")\n",
    "        return [], [], {}\n",
    "    \n",
    "    print(\"Loading therapeutic peptides...\")\n",
    "    therapeutic_total = 0\n",
    "    for filename in os.listdir(therapeutic_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(therapeutic_path, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                if df.shape[1] >= 1:\n",
    "                    seqs = df.iloc[:, 0].dropna().astype(str).tolist()\n",
    "                    seqs = [seq.strip().upper() for seq in seqs if seq.strip()]\n",
    "                    \n",
    "                    valid_seqs = []\n",
    "                    for seq in seqs:\n",
    "                        clean_seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "                        if 5 <= len(clean_seq) <= 200:\n",
    "                            valid_seqs.append(clean_seq)\n",
    "                    \n",
    "                    # Limit sequences if max_sequences specified\n",
    "                    if max_sequences and len(sequences) + len(valid_seqs) > max_sequences // 2:\n",
    "                        remaining = max(0, max_sequences // 2 - len(sequences))\n",
    "                        valid_seqs = valid_seqs[:remaining]\n",
    "                    \n",
    "                    sequences.extend(valid_seqs)\n",
    "                    labels.extend([1] * len(valid_seqs))\n",
    "                    therapeutic_total += len(valid_seqs)\n",
    "                    file_stats[f\"therapeutic_{filename}\"] = len(valid_seqs)\n",
    "                    print(f\"  ‚úì {filename}: {len(valid_seqs)} valid sequences\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Error reading {filename}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Total therapeutic sequences: {therapeutic_total}\")\n",
    "    \n",
    "    print(\"\\nLoading non-therapeutic peptides...\")\n",
    "    non_therapeutic_total = 0\n",
    "    for filename in os.listdir(non_therapeutic_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(non_therapeutic_path, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                if df.shape[1] >= 1:\n",
    "                    seqs = df.iloc[:, 0].dropna().astype(str).tolist()\n",
    "                    seqs = [seq.strip().upper() for seq in seqs if seq.strip()]\n",
    "                    \n",
    "                    valid_seqs = []\n",
    "                    for seq in seqs:\n",
    "                        clean_seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "                        if 5 <= len(clean_seq) <= 200:\n",
    "                            valid_seqs.append(clean_seq)\n",
    "                    \n",
    "                    # Limit sequences if max_sequences specified\n",
    "                    if max_sequences and len([l for l in labels if l == 0]) + len(valid_seqs) > max_sequences // 2:\n",
    "                        remaining = max(0, max_sequences // 2 - len([l for l in labels if l == 0]))\n",
    "                        valid_seqs = valid_seqs[:remaining]\n",
    "                    \n",
    "                    sequences.extend(valid_seqs)\n",
    "                    labels.extend([0] * len(valid_seqs))\n",
    "                    non_therapeutic_total += len(valid_seqs)\n",
    "                    file_stats[f\"non_therapeutic_{filename}\"] = len(valid_seqs)\n",
    "                    print(f\"  ‚úì {filename}: {len(valid_seqs)} valid sequences\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Error reading {filename}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Total non-therapeutic sequences: {non_therapeutic_total}\")\n",
    "    print(f\"Overall total: {len(sequences)} sequences\")\n",
    "    \n",
    "    return sequences, labels, file_stats\n",
    "\n",
    "def preprocess_sequences(sequences, labels):\n",
    "    valid_data = []\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if 5 <= len(seq) <= 100 and all(aa in 'ACDEFGHIKLMNPQRSTVWY' for aa in seq):\n",
    "            valid_data.append((seq, labels[i]))\n",
    "    \n",
    "    sequences, labels = zip(*valid_data) if valid_data else ([], [])\n",
    "    return list(sequences), list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbfca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statistical_features(sequences):\n",
    "    \"\"\"\n",
    "    Extract comprehensive statistical features from peptide sequences\n",
    "    Returns enhanced feature set for better model performance\n",
    "    \"\"\"\n",
    "    def calc_features(seq):\n",
    "        length = len(seq)\n",
    "        aa_counts = {char: seq.count(char) for char in \"ACDEFGHIKLMNPQRSTVWY\"}\n",
    "        \n",
    "        # Amino acid group compositions\n",
    "        hydrophobic = \"AILMFWV\"\n",
    "        polar = \"NQST\"\n",
    "        charged = \"KRDEH\"\n",
    "        aromatic = \"FWY\"\n",
    "        tiny = \"ACSV\"\n",
    "        small = \"ABDHNT\"\n",
    "        aliphatic = \"ILV\"\n",
    "        \n",
    "        # Calculate group compositions\n",
    "        hydrophobicity = sum(aa_counts.get(char, 0) for char in hydrophobic) / length\n",
    "        polarity = sum(aa_counts.get(char, 0) for char in polar) / length\n",
    "        aromaticity = sum(aa_counts.get(char, 0) for char in aromatic) / length\n",
    "        tiny_fraction = sum(aa_counts.get(char, 0) for char in tiny) / length\n",
    "        small_fraction = sum(aa_counts.get(char, 0) for char in small) / length\n",
    "        aliphatic_fraction = sum(aa_counts.get(char, 0) for char in aliphatic) / length\n",
    "        \n",
    "        # Charge properties\n",
    "        positive_charge = aa_counts.get('K', 0) + aa_counts.get('R', 0) + aa_counts.get('H', 0)\n",
    "        negative_charge = aa_counts.get('D', 0) + aa_counts.get('E', 0)\n",
    "        net_charge = positive_charge - negative_charge\n",
    "        \n",
    "        # Structural features\n",
    "        proline_content = aa_counts.get('P', 0) / length\n",
    "        glycine_content = aa_counts.get('G', 0) / length\n",
    "        cysteine_content = aa_counts.get('C', 0) / length\n",
    "        \n",
    "        # Hydrophobic moment calculation\n",
    "        hydrophobic_values = {\n",
    "            'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,\n",
    "            'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,\n",
    "            'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,\n",
    "            'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2\n",
    "        }\n",
    "        \n",
    "        hydrophobic_moment = 0\n",
    "        for i, aa in enumerate(seq):\n",
    "            if aa in hydrophobic_values:\n",
    "                angle = i * 100 * np.pi / 180\n",
    "                hydrophobic_moment += hydrophobic_values[aa] * np.exp(1j * angle)\n",
    "        \n",
    "        return [\n",
    "            length, hydrophobicity, polarity, net_charge/length, aromaticity,\n",
    "            tiny_fraction, small_fraction, aliphatic_fraction,\n",
    "            positive_charge/length, negative_charge/length, abs(net_charge)/length,\n",
    "            proline_content, glycine_content, cysteine_content,\n",
    "            abs(hydrophobic_moment)/length\n",
    "        ]\n",
    "    \n",
    "    return np.array([calc_features(seq) for seq in sequences], dtype=np.float32)\n",
    "\n",
    "def get_protbert_embeddings(sequences, batch_size=8):\n",
    "    \"\"\"\n",
    "    Extract ProtBERT embeddings with optimized memory usage and error handling\n",
    "    \"\"\"\n",
    "    protbert_model.eval()\n",
    "    embeddings = []\n",
    "    failed_sequences = 0\n",
    "    \n",
    "    print(f\"Extracting ProtBERT embeddings for {len(sequences)} sequences...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(sequences), batch_size), desc=\"ProtBERT Embedding\"):\n",
    "            try:\n",
    "                batch_sequences = sequences[i:i + batch_size]\n",
    "                batch = [\" \".join(list(seq)) for seq in batch_sequences]\n",
    "                \n",
    "                inputs = tokenizer(\n",
    "                    batch, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=512\n",
    "                ).to(device)\n",
    "                \n",
    "                outputs = protbert_model(**inputs)\n",
    "                batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "                embeddings.append(batch_embeddings)\n",
    "                \n",
    "                # Clear cache periodically\n",
    "                if i % (batch_size * 10) == 0:\n",
    "                    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {i//batch_size}: {e}\")\n",
    "                failed_sequences += len(batch_sequences)\n",
    "                # Add zero embeddings for failed sequences\n",
    "                zero_embeddings = np.zeros((len(batch_sequences), 1024), dtype=np.float32)\n",
    "                embeddings.append(zero_embeddings)\n",
    "                continue\n",
    "    \n",
    "    if failed_sequences > 0:\n",
    "        print(f\"Warning: Failed to process {failed_sequences} sequences\")\n",
    "    \n",
    "    final_embeddings = np.vstack(embeddings).astype(np.float32)\n",
    "    print(f\"Generated embeddings shape: {final_embeddings.shape}\")\n",
    "    \n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71987b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_multimodal_model(embedding_dim, statistical_dim):\n",
    "    \"\"\"\n",
    "    Create advanced multi-modal neural network combining ProtBERT embeddings and statistical features\n",
    "    \"\"\"\n",
    "    # Input layers\n",
    "    embedding_input = Input(shape=(embedding_dim,), name='protbert_embeddings')\n",
    "    statistical_input = Input(shape=(statistical_dim,), name='statistical_features')\n",
    "    \n",
    "    # ProtBERT embedding branch\n",
    "    emb_branch = Dense(512, activation='relu')(embedding_input)\n",
    "    emb_branch = BatchNormalization()(emb_branch)\n",
    "    emb_branch = Dropout(0.3)(emb_branch)\n",
    "    \n",
    "    emb_branch = Dense(256, activation='relu')(emb_branch)\n",
    "    emb_branch = BatchNormalization()(emb_branch)\n",
    "    emb_branch = Dropout(0.2)(emb_branch)\n",
    "    \n",
    "    # Statistical features branch\n",
    "    stat_branch = Dense(128, activation='relu')(statistical_input)\n",
    "    stat_branch = BatchNormalization()(stat_branch)\n",
    "    stat_branch = Dropout(0.3)(stat_branch)\n",
    "    \n",
    "    stat_branch = Dense(64, activation='relu')(stat_branch)\n",
    "    stat_branch = Dropout(0.2)(stat_branch)\n",
    "    \n",
    "    # Fusion layer\n",
    "    combined = concatenate([emb_branch, stat_branch], name='feature_fusion')\n",
    "    \n",
    "    # Final classification layers\n",
    "    dense1 = Dense(512, activation='relu')(combined)\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    dense1 = Dropout(0.4)(dense1)\n",
    "    \n",
    "    dense2 = Dense(256, activation='relu')(dense1)\n",
    "    dense2 = BatchNormalization()(dense2)\n",
    "    dense2 = Dropout(0.3)(dense2)\n",
    "    \n",
    "    dense3 = Dense(128, activation='relu')(dense2)\n",
    "    dense3 = BatchNormalization()(dense3)\n",
    "    dense3 = Dropout(0.2)(dense3)\n",
    "    \n",
    "    dense4 = Dense(64, activation='relu')(dense3)\n",
    "    dense4 = Dropout(0.1)(dense4)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(1, activation='sigmoid', name='therapeutic_prediction')(dense4)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = Model(inputs=[embedding_input, statistical_input], outputs=output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0af6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ADVANCED CNN-LSTM WITH PROTBERT FOR THERAPEUTIC PEPTIDE PREDICTION ===\\n\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(\"../backend/models\", exist_ok=True)\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# Load data with correct relative paths\n",
    "data_path = \"../data\"\n",
    "sequences, labels, file_stats = load_peptide_data(data_path, max_sequences=10000)\n",
    "sequences, labels = preprocess_sequences(sequences, labels)\n",
    "\n",
    "# Display dataset statistics\n",
    "if file_stats:\n",
    "    print(\"\\nüìä Dataset Statistics by File:\")\n",
    "    for file_key, count in file_stats.items():\n",
    "        print(f\"  {file_key}: {count:,} sequences\")\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(sequences)} sequences\")\n",
    "print(f\"Therapeutic: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "print(f\"Non-therapeutic: {len(labels) - sum(labels)} ({(len(labels) - sum(labels))/len(labels)*100:.1f}%)\")\n",
    "\n",
    "if len(sequences) == 0:\n",
    "    print(\"ERROR: No valid sequences found!\")\n",
    "    print(\"Please check your data files in ../data/Therapeutic data/ and ../data/Non-Therapeutic data/\")\n",
    "    exit()\n",
    "\n",
    "# Handle ProtBERT embeddings with caching\n",
    "embeddings_file = \"../data/processed/protbert_embeddings.npy\"\n",
    "labels_file = \"../data/processed/protbert_labels.npy\"\n",
    "\n",
    "if os.path.exists(embeddings_file) and os.path.exists(labels_file):\n",
    "    print(\"\\nüìÅ Loading cached ProtBERT embeddings...\")\n",
    "    protbert_embeddings = np.load(embeddings_file)\n",
    "    cached_labels = np.load(labels_file)\n",
    "    \n",
    "    # Verify cache consistency\n",
    "    if len(protbert_embeddings) == len(sequences) and np.array_equal(cached_labels, labels):\n",
    "        print(\"‚úÖ Cache is valid and consistent\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Cache is inconsistent, regenerating embeddings...\")\n",
    "        protbert_embeddings = get_protbert_embeddings(sequences)\n",
    "        np.save(embeddings_file, protbert_embeddings)\n",
    "        np.save(labels_file, np.array(labels))\n",
    "else:\n",
    "    print(\"\\nüß¨ Extracting ProtBERT embeddings (this may take a while)...\")\n",
    "    protbert_embeddings = get_protbert_embeddings(sequences)\n",
    "    print(\"üíæ Caching embeddings for future use...\")\n",
    "    np.save(embeddings_file, protbert_embeddings)\n",
    "    np.save(labels_file, np.array(labels))\n",
    "\n",
    "# Extract enhanced statistical features\n",
    "print(\"\\nüìà Extracting comprehensive statistical features...\")\n",
    "statistical_features = extract_statistical_features(sequences)\n",
    "print(f\"Statistical features shape: {statistical_features.shape}\")\n",
    "\n",
    "# Feature scaling\n",
    "print(\"\\n‚öñÔ∏è  Scaling features...\")\n",
    "scaler_protbert = StandardScaler()\n",
    "scaler_statistical = StandardScaler()\n",
    "\n",
    "protbert_scaled = scaler_protbert.fit_transform(protbert_embeddings)\n",
    "statistical_scaled = scaler_statistical.fit_transform(statistical_features)\n",
    "\n",
    "# Save scalers\n",
    "with open(\"../backend/models/protbert_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_protbert, f)\n",
    "with open(\"../backend/models/statistical_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_statistical, f)\n",
    "print(\"‚úÖ Scalers saved to ../backend/models/\")\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train_emb, X_test_emb, X_train_stat, X_test_stat, y_train, y_test = train_test_split(\n",
    "    protbert_scaled, statistical_scaled, labels, \n",
    "    test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Split:\")\n",
    "print(f\"  Training: {len(X_train_emb)} samples\")\n",
    "print(f\"  Testing: {len(X_test_emb)} samples\")\n",
    "print(f\"  ProtBERT features: {X_train_emb.shape[1]}\")\n",
    "print(f\"  Statistical features: {X_train_stat.shape[1]}\")\n",
    "\n",
    "# Build advanced model\n",
    "print(\"\\nüèóÔ∏è  Building advanced multi-modal neural network...\")\n",
    "model = create_advanced_multimodal_model(protbert_scaled.shape[1], statistical_scaled.shape[1])\n",
    "model.summary()\n",
    "\n",
    "# Configure callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=15, \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../backend/models/advanced_protbert_model.h5', \n",
    "        save_best_only=True, \n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Training advanced model...\")\n",
    "history = model.fit(\n",
    "    [X_train_emb, X_train_stat], y_train,\n",
    "    validation_data=([X_test_emb, X_test_stat], y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Evaluating model performance...\")\n",
    "test_results = model.evaluate([X_test_emb, X_test_stat], y_test, verbose=0)\n",
    "test_loss = test_results[0]\n",
    "test_accuracy = test_results[1]\n",
    "test_precision = test_results[2] if len(test_results) > 2 else 0\n",
    "test_recall = test_results[3] if len(test_results) > 3 else 0\n",
    "\n",
    "# Calculate additional metrics\n",
    "y_pred = model.predict([X_test_emb, X_test_stat], verbose=0)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
    "\n",
    "sklearn_precision = precision_score(y_test, y_pred_binary)\n",
    "sklearn_recall = recall_score(y_test, y_pred_binary)\n",
    "sklearn_f1 = f1_score(y_test, y_pred_binary)\n",
    "sklearn_accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f\"\\n=== FINAL MODEL PERFORMANCE ===\")\n",
    "print(f\"Test Accuracy: {sklearn_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {sklearn_precision:.4f}\")\n",
    "print(f\"Test Recall: {sklearn_recall:.4f}\")\n",
    "print(f\"F1-Score: {sklearn_f1:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Enhanced visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "plt.title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('Model Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "values = [sklearn_accuracy, sklearn_precision, sklearn_recall, sklearn_f1]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "bars = plt.bar(metrics, values, color=colors, alpha=0.8)\n",
    "plt.title('Model Performance Metrics', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Model training completed!\")\n",
    "print(f\"üìÅ Model saved: ../backend/models/advanced_protbert_model.h5\")\n",
    "print(f\"üìÅ Scalers saved: ../backend/models/protbert_scaler.pkl & statistical_scaler.pkl\")\n",
    "print(f\"üíæ Embeddings cached: ../data/processed/protbert_embeddings.npy\")\n",
    "\n",
    "print(f\"\\nüî¨ Model Architecture Summary:\")\n",
    "print(f\"  - ProtBERT embeddings: {protbert_scaled.shape[1]} features\")\n",
    "print(f\"  - Statistical features: {statistical_scaled.shape[1]} features\")\n",
    "print(f\"  - Multi-modal fusion with advanced regularization\")\n",
    "print(f\"  - Total parameters: {model.count_params():,}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
