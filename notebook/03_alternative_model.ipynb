{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "def load_peptide_data(data_path, max_sequences_per_class=25000):\n",
    "    \"\"\"\n",
    "    Load peptide sequences from CSV files with comprehensive error handling and statistics\n",
    "    \"\"\"\n",
    "    therapeutic_path = os.path.join(data_path, \"Therapeutic data\")\n",
    "    non_therapeutic_path = os.path.join(data_path, \"Non-Therapeutic data\")\n",
    "    \n",
    "    sequences, labels = [], []\n",
    "    file_stats = {}\n",
    "    \n",
    "    if not os.path.exists(therapeutic_path):\n",
    "        print(f\"Warning: {therapeutic_path} not found!\")\n",
    "        return [], [], {}\n",
    "    \n",
    "    if not os.path.exists(non_therapeutic_path):\n",
    "        print(f\"Warning: {non_therapeutic_path} not found!\")\n",
    "        return [], [], {}\n",
    "    \n",
    "    print(\"Loading therapeutic peptides...\")\n",
    "    therapeutic_total = 0\n",
    "    for filename in os.listdir(therapeutic_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(therapeutic_path, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                if df.shape[1] >= 1:\n",
    "                    seqs = df.iloc[:, 0].dropna().astype(str).tolist()\n",
    "                    seqs = [seq.strip().upper() for seq in seqs if seq.strip()]\n",
    "                    \n",
    "                    # Validate and clean sequences\n",
    "                    valid_seqs = []\n",
    "                    for seq in seqs:\n",
    "                        clean_seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "                        if 5 <= len(clean_seq) <= 200:\n",
    "                            valid_seqs.append(clean_seq)\n",
    "                    \n",
    "                    # Limit sequences to prevent memory issues\n",
    "                    if max_sequences_per_class and therapeutic_total + len(valid_seqs) > max_sequences_per_class:\n",
    "                        remaining = max(0, max_sequences_per_class - therapeutic_total)\n",
    "                        valid_seqs = valid_seqs[:remaining]\n",
    "                    \n",
    "                    sequences.extend(valid_seqs)\n",
    "                    labels.extend([1] * len(valid_seqs))\n",
    "                    therapeutic_total += len(valid_seqs)\n",
    "                    file_stats[f\"therapeutic_{filename}\"] = len(valid_seqs)\n",
    "                    print(f\"  ‚úì {filename}: {len(valid_seqs)} valid sequences\")\n",
    "                    \n",
    "                    if max_sequences_per_class and therapeutic_total >= max_sequences_per_class:\n",
    "                        break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Error reading {filename}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Total therapeutic sequences: {therapeutic_total}\")\n",
    "    \n",
    "    print(\"\\nLoading non-therapeutic peptides...\")\n",
    "    non_therapeutic_total = 0\n",
    "    for filename in os.listdir(non_therapeutic_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(non_therapeutic_path, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                if df.shape[1] >= 1:\n",
    "                    seqs = df.iloc[:, 0].dropna().astype(str).tolist()\n",
    "                    seqs = [seq.strip().upper() for seq in seqs if seq.strip()]\n",
    "                    \n",
    "                    # Validate and clean sequences\n",
    "                    valid_seqs = []\n",
    "                    for seq in seqs:\n",
    "                        clean_seq = ''.join([c for c in seq if c in 'ACDEFGHIKLMNPQRSTVWY'])\n",
    "                        if 5 <= len(clean_seq) <= 200:\n",
    "                            valid_seqs.append(clean_seq)\n",
    "                    \n",
    "                    # Limit sequences to prevent memory issues\n",
    "                    if max_sequences_per_class and non_therapeutic_total + len(valid_seqs) > max_sequences_per_class:\n",
    "                        remaining = max(0, max_sequences_per_class - non_therapeutic_total)\n",
    "                        valid_seqs = valid_seqs[:remaining]\n",
    "                    \n",
    "                    sequences.extend(valid_seqs)\n",
    "                    labels.extend([0] * len(valid_seqs))\n",
    "                    non_therapeutic_total += len(valid_seqs)\n",
    "                    file_stats[f\"non_therapeutic_{filename}\"] = len(valid_seqs)\n",
    "                    print(f\"  ‚úì {filename}: {len(valid_seqs)} valid sequences\")\n",
    "                    \n",
    "                    if max_sequences_per_class and non_therapeutic_total >= max_sequences_per_class:\n",
    "                        break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Error reading {filename}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Total non-therapeutic sequences: {non_therapeutic_total}\")\n",
    "    print(f\"Overall total: {len(sequences)} sequences\")\n",
    "    \n",
    "    return sequences, labels, file_stats\n",
    "\n",
    "def extract_comprehensive_features(sequences):\n",
    "    \"\"\"\n",
    "    Extract comprehensive biochemical features from peptide sequences\n",
    "    Returns enhanced feature set optimized for alternative model comparison\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    feature_names = []\n",
    "    \n",
    "    print(\"Extracting comprehensive biochemical features...\")\n",
    "    \n",
    "    for seq in tqdm(sequences, desc=\"Processing sequences\"):\n",
    "        seq_len = len(seq)\n",
    "        feature_vector = []\n",
    "        \n",
    "        # 1. Basic sequence properties\n",
    "        feature_vector.append(seq_len)\n",
    "        \n",
    "        # 2. Amino acid composition (20 features)\n",
    "        aa_composition = {}\n",
    "        for aa in 'ACDEFGHIKLMNPQRSTVWY':\n",
    "            aa_composition[aa] = seq.count(aa) / seq_len\n",
    "            feature_vector.append(aa_composition[aa])\n",
    "        \n",
    "        # 3. Amino acid group compositions\n",
    "        hydrophobic = sum(seq.count(aa) for aa in 'AILMFWV') / seq_len\n",
    "        polar = sum(seq.count(aa) for aa in 'NQST') / seq_len\n",
    "        charged = sum(seq.count(aa) for aa in 'KRDEH') / seq_len\n",
    "        aromatic = sum(seq.count(aa) for aa in 'FWY') / seq_len\n",
    "        tiny = sum(seq.count(aa) for aa in 'ACSV') / seq_len\n",
    "        small = sum(seq.count(aa) for aa in 'ABDHNT') / seq_len\n",
    "        aliphatic = sum(seq.count(aa) for aa in 'ILV') / seq_len\n",
    "        \n",
    "        feature_vector.extend([hydrophobic, polar, charged, aromatic, tiny, small, aliphatic])\n",
    "        \n",
    "        # 4. Charge properties\n",
    "        positive_charge = seq.count('K') + seq.count('R') + seq.count('H')\n",
    "        negative_charge = seq.count('D') + seq.count('E')\n",
    "        net_charge = positive_charge - negative_charge\n",
    "        \n",
    "        feature_vector.extend([\n",
    "            positive_charge / seq_len,\n",
    "            negative_charge / seq_len,\n",
    "            net_charge / seq_len,\n",
    "            abs(net_charge) / seq_len\n",
    "        ])\n",
    "        \n",
    "        # 5. Structural features\n",
    "        proline_content = seq.count('P') / seq_len\n",
    "        glycine_content = seq.count('G') / seq_len\n",
    "        cysteine_content = seq.count('C') / seq_len\n",
    "        \n",
    "        feature_vector.extend([proline_content, glycine_content, cysteine_content])\n",
    "        \n",
    "        # 6. Dipeptide composition (enhanced)\n",
    "        dipeptide_freq = {}\n",
    "        for i in range(len(seq)-1):\n",
    "            dipeptide = seq[i:i+2]\n",
    "            dipeptide_freq[dipeptide] = dipeptide_freq.get(dipeptide, 0) + 1\n",
    "        \n",
    "        # Top 15 most important dipeptides for therapeutic prediction\n",
    "        important_dipeptides = ['AA', 'AC', 'AG', 'AL', 'AR', 'AS', 'AT', 'AV', 'AY', 'AW',\n",
    "                               'KK', 'RR', 'LL', 'FF', 'WW']\n",
    "        \n",
    "        for dp in important_dipeptides:\n",
    "            feature_vector.append(dipeptide_freq.get(dp, 0) / (seq_len - 1) if seq_len > 1 else 0)\n",
    "        \n",
    "        # 7. Hydrophobic moment (simplified)\n",
    "        hydrophobic_values = {\n",
    "            'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,\n",
    "            'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,\n",
    "            'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,\n",
    "            'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2\n",
    "        }\n",
    "        \n",
    "        hydrophobic_moment = 0\n",
    "        for i, aa in enumerate(seq):\n",
    "            if aa in hydrophobic_values:\n",
    "                angle = i * 100 * np.pi / 180\n",
    "                hydrophobic_moment += hydrophobic_values[aa] * np.exp(1j * angle)\n",
    "        \n",
    "        feature_vector.append(abs(hydrophobic_moment) / seq_len)\n",
    "        \n",
    "        # 8. Sequence complexity measures\n",
    "        # Repeating patterns\n",
    "        max_repeat = 1\n",
    "        current_repeat = 1\n",
    "        for i in range(1, seq_len):\n",
    "            if seq[i] == seq[i-1]:\n",
    "                current_repeat += 1\n",
    "                max_repeat = max(max_repeat, current_repeat)\n",
    "            else:\n",
    "                current_repeat = 1\n",
    "        \n",
    "        feature_vector.append(max_repeat / seq_len)\n",
    "        \n",
    "        # Entropy (sequence diversity)\n",
    "        entropy = -sum((count/seq_len) * np.log2(count/seq_len) \n",
    "                      for count in aa_composition.values() if count > 0)\n",
    "        feature_vector.append(entropy)\n",
    "        \n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    features_array = np.array(features, dtype=np.float32)\n",
    "    print(f\"Feature extraction completed: {features_array.shape[1]} features per sequence\")\n",
    "    \n",
    "    return features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecee9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_dense_model(input_dim):\n",
    "    \"\"\"\n",
    "    Create advanced dense neural network with batch normalization and regularization\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(input_dim,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_advanced_conv1d_model(input_dim):\n",
    "    \"\"\"\n",
    "    Create advanced 1D CNN model with enhanced architecture\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Reshape((input_dim, 1), input_shape=(input_dim,)),\n",
    "        \n",
    "        Conv1D(128, 5, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv1D(64, 3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv1D(32, 3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_lstm_model(input_dim):\n",
    "    \"\"\"\n",
    "    Create LSTM model for sequence-based learning\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Reshape((input_dim, 1), input_shape=(input_dim,)),\n",
    "        \n",
    "        LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ALTERNATIVE MODEL COMPARISON FOR THERAPEUTIC PEPTIDE PREDICTION ===\\n\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(\"../backend/models\", exist_ok=True)\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# Load data with correct relative paths\n",
    "data_path = \"../data\"\n",
    "sequences, labels, file_stats = load_peptide_data(data_path, max_sequences_per_class=15000)\n",
    "\n",
    "# Display dataset statistics\n",
    "if file_stats:\n",
    "    print(\"\\nüìä Dataset Statistics by File:\")\n",
    "    for file_key, count in file_stats.items():\n",
    "        print(f\"  {file_key}: {count:,} sequences\")\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(sequences)} sequences\")\n",
    "print(f\"Therapeutic: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "print(f\"Non-therapeutic: {len(labels) - sum(labels)} ({(len(labels) - sum(labels))/len(labels)*100:.1f}%)\")\n",
    "\n",
    "if len(sequences) == 0:\n",
    "    print(\"ERROR: No valid sequences found!\")\n",
    "    print(\"Please check your data files in ../data/Therapeutic data/ and ../data/Non-Therapeutic data/\")\n",
    "    exit()\n",
    "\n",
    "# Extract comprehensive features\n",
    "features = extract_comprehensive_features(sequences)\n",
    "print(f\"Feature matrix shape: {features.shape}\")\n",
    "\n",
    "# Feature scaling\n",
    "print(\"\\n‚öñÔ∏è  Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Save scaler\n",
    "with open(\"../backend/models/alternative_models_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"‚úÖ Scaler saved to ../backend/models/alternative_models_scaler.pkl\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_scaled, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Split:\")\n",
    "print(f\"  Training: {len(X_train)} samples\")\n",
    "print(f\"  Testing: {len(X_test)} samples\")\n",
    "print(f\"  Features: {features_scaled.shape[1]}\")\n",
    "\n",
    "# Enhanced callbacks\n",
    "callbacks_dense = [\n",
    "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('../backend/models/advanced_dense_model.h5', save_best_only=True, monitor='val_loss', verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "callbacks_conv = [\n",
    "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('../backend/models/advanced_conv1d_model.h5', save_best_only=True, monitor='val_loss', verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "callbacks_lstm = [\n",
    "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('../backend/models/advanced_lstm_model.h5', save_best_only=True, monitor='val_loss', verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "# Train models and store results\n",
    "model_results = {}\n",
    "\n",
    "print(\"\\nüèóÔ∏è  Training Advanced Dense Neural Network...\")\n",
    "dense_model = create_advanced_dense_model(features_scaled.shape[1])\n",
    "dense_model.summary()\n",
    "\n",
    "dense_history = dense_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks_dense,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dense_results = dense_model.evaluate(X_test, y_test, verbose=0)\n",
    "dense_acc, dense_prec, dense_rec = dense_results[1], dense_results[2], dense_results[3]\n",
    "dense_f1 = 2 * (dense_prec * dense_rec) / (dense_prec + dense_rec) if (dense_prec + dense_rec) > 0 else 0\n",
    "model_results['Advanced Dense NN'] = {'accuracy': dense_acc, 'precision': dense_prec, 'recall': dense_rec, 'f1': dense_f1}\n",
    "\n",
    "print(f\"‚úÖ Dense Model - Accuracy: {dense_acc:.4f}, Precision: {dense_prec:.4f}, Recall: {dense_rec:.4f}, F1: {dense_f1:.4f}\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è  Training Advanced Conv1D Model...\")\n",
    "conv_model = create_advanced_conv1d_model(features_scaled.shape[1])\n",
    "\n",
    "conv_history = conv_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks_conv,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "conv_results = conv_model.evaluate(X_test, y_test, verbose=0)\n",
    "conv_acc, conv_prec, conv_rec = conv_results[1], conv_results[2], conv_results[3]\n",
    "conv_f1 = 2 * (conv_prec * conv_rec) / (conv_prec + conv_rec) if (conv_prec + conv_rec) > 0 else 0\n",
    "model_results['Advanced Conv1D'] = {'accuracy': conv_acc, 'precision': conv_prec, 'recall': conv_rec, 'f1': conv_f1}\n",
    "\n",
    "print(f\"‚úÖ Conv1D Model - Accuracy: {conv_acc:.4f}, Precision: {conv_prec:.4f}, Recall: {conv_rec:.4f}, F1: {conv_f1:.4f}\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è  Training Advanced LSTM Model...\")\n",
    "lstm_model = create_lstm_model(features_scaled.shape[1])\n",
    "\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks_lstm,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lstm_results = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
    "lstm_acc, lstm_prec, lstm_rec = lstm_results[1], lstm_results[2], lstm_results[3]\n",
    "lstm_f1 = 2 * (lstm_prec * lstm_rec) / (lstm_prec + lstm_rec) if (lstm_prec + lstm_rec) > 0 else 0\n",
    "model_results['Advanced LSTM'] = {'accuracy': lstm_acc, 'precision': lstm_prec, 'recall': lstm_rec, 'f1': lstm_f1}\n",
    "\n",
    "print(f\"‚úÖ LSTM Model - Accuracy: {lstm_acc:.4f}, Precision: {lstm_prec:.4f}, Recall: {lstm_rec:.4f}, F1: {lstm_f1:.4f}\")\n",
    "\n",
    "print(\"\\nüå≤ Training Random Forest Classifier...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300, \n",
    "    max_depth=25, \n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "rf_prec = precision_score(y_test, y_pred_rf)\n",
    "rf_rec = recall_score(y_test, y_pred_rf)\n",
    "rf_f1 = f1_score(y_test, y_pred_rf)\n",
    "model_results['Random Forest'] = {'accuracy': rf_acc, 'precision': rf_prec, 'recall': rf_rec, 'f1': rf_f1}\n",
    "\n",
    "with open(\"../backend/models/random_forest_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "print(f\"‚úÖ Random Forest - Accuracy: {rf_acc:.4f}, Precision: {rf_prec:.4f}, Recall: {rf_rec:.4f}, F1: {rf_f1:.4f}\")\n",
    "\n",
    "print(\"\\nüöÄ Training Gradient Boosting Classifier...\")\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=8,\n",
    "    random_state=42\n",
    ")\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "gb_acc = accuracy_score(y_test, y_pred_gb)\n",
    "gb_prec = precision_score(y_test, y_pred_gb)\n",
    "gb_rec = recall_score(y_test, y_pred_gb)\n",
    "gb_f1 = f1_score(y_test, y_pred_gb)\n",
    "model_results['Gradient Boosting'] = {'accuracy': gb_acc, 'precision': gb_prec, 'recall': gb_rec, 'f1': gb_f1}\n",
    "\n",
    "with open(\"../backend/models/gradient_boosting_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gb_model, f)\n",
    "\n",
    "print(f\"‚úÖ Gradient Boosting - Accuracy: {gb_acc:.4f}, Precision: {gb_prec:.4f}, Recall: {gb_rec:.4f}, F1: {gb_f1:.4f}\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è  Training SVM Classifier...\")\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42, probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "svm_acc = accuracy_score(y_test, y_pred_svm)\n",
    "svm_prec = precision_score(y_test, y_pred_svm)\n",
    "svm_rec = recall_score(y_test, y_pred_svm)\n",
    "svm_f1 = f1_score(y_test, y_pred_svm)\n",
    "model_results['SVM (RBF)'] = {'accuracy': svm_acc, 'precision': svm_prec, 'recall': svm_rec, 'f1': svm_f1}\n",
    "\n",
    "with open(\"../backend/models/svm_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(svm_model, f)\n",
    "\n",
    "print(f\"‚úÖ SVM - Accuracy: {svm_acc:.4f}, Precision: {svm_prec:.4f}, Recall: {svm_rec:.4f}, F1: {svm_f1:.4f}\")\n",
    "\n",
    "print(\"\\nüìä Detailed Classification Reports:\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RANDOM FOREST CLASSIFICATION REPORT:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GRADIENT BOOSTING CLASSIFICATION REPORT:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7428d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Visualization and Analysis\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "model_names = list(model_results.keys())\n",
    "x_pos = np.arange(len(model_names))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [model_results[model][metric] for model in model_names]\n",
    "    axes[0, 0].bar(x_pos + i*0.2, values, width=0.2, label=metric.capitalize(), alpha=0.8)\n",
    "\n",
    "axes[0, 0].set_xlabel('Models')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xticks(x_pos + 0.3)\n",
    "axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# 2. Training History - Accuracy\n",
    "axes[0, 1].plot(dense_history.history['accuracy'], label='Dense Train', linewidth=2)\n",
    "axes[0, 1].plot(dense_history.history['val_accuracy'], label='Dense Val', linewidth=2)\n",
    "axes[0, 1].plot(conv_history.history['accuracy'], label='Conv1D Train', linewidth=2)\n",
    "axes[0, 1].plot(conv_history.history['val_accuracy'], label='Conv1D Val', linewidth=2)\n",
    "axes[0, 1].plot(lstm_history.history['accuracy'], label='LSTM Train', linewidth=2)\n",
    "axes[0, 1].plot(lstm_history.history['val_accuracy'], label='LSTM Val', linewidth=2)\n",
    "axes[0, 1].set_title('Neural Network Training - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Training History - Loss\n",
    "axes[0, 2].plot(dense_history.history['loss'], label='Dense Train', linewidth=2)\n",
    "axes[0, 2].plot(dense_history.history['val_loss'], label='Dense Val', linewidth=2)\n",
    "axes[0, 2].plot(conv_history.history['loss'], label='Conv1D Train', linewidth=2)\n",
    "axes[0, 2].plot(conv_history.history['val_loss'], label='Conv1D Val', linewidth=2)\n",
    "axes[0, 2].plot(lstm_history.history['loss'], label='LSTM Train', linewidth=2)\n",
    "axes[0, 2].plot(lstm_history.history['val_loss'], label='LSTM Val', linewidth=2)\n",
    "axes[0, 2].set_title('Neural Network Training - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('Loss')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature Importance (Random Forest)\n",
    "feature_importance = rf_model.feature_importances_\n",
    "top_indices = np.argsort(feature_importance)[-25:]\n",
    "axes[1, 0].barh(range(25), feature_importance[top_indices], alpha=0.8, color='forestgreen')\n",
    "axes[1, 0].set_title('Random Forest - Top 25 Feature Importance', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Importance')\n",
    "axes[1, 0].set_ylabel('Feature Index')\n",
    "\n",
    "# 5. Model Accuracy Heatmap\n",
    "accuracy_matrix = np.array([[model_results[model]['accuracy'] for model in model_names]])\n",
    "im = axes[1, 1].imshow(accuracy_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "axes[1, 1].set_title('Model Accuracy Heatmap', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xticks(range(len(model_names)))\n",
    "axes[1, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[1, 1].set_yticks([0])\n",
    "axes[1, 1].set_yticklabels(['Accuracy'])\n",
    "\n",
    "# Add text annotations\n",
    "for i, model in enumerate(model_names):\n",
    "    axes[1, 1].text(i, 0, f'{model_results[model][\"accuracy\"]:.3f}', \n",
    "                   ha='center', va='center', fontweight='bold', color='black')\n",
    "\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "# 6. F1-Score Comparison\n",
    "f1_scores = [model_results[model]['f1'] for model in model_names]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))\n",
    "bars = axes[1, 2].bar(model_names, f1_scores, color=colors, alpha=0.8)\n",
    "axes[1, 2].set_title('F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_ylabel('F1-Score')\n",
    "axes[1, 2].set_ylim(0, 1)\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, f1 in zip(bars, f1_scores):\n",
    "    axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                   f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance Summary Table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üèÜ COMPREHENSIVE MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Model':<20} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for model_name, metrics in model_results.items():\n",
    "    print(f\"{model_name:<20} {metrics['accuracy']:<12.4f} {metrics['precision']:<12.4f} \"\n",
    "          f\"{metrics['recall']:<12.4f} {metrics['f1']:<12.4f}\")\n",
    "\n",
    "# Find best models for each metric\n",
    "best_accuracy = max(model_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "best_precision = max(model_results.items(), key=lambda x: x[1]['precision'])\n",
    "best_recall = max(model_results.items(), key=lambda x: x[1]['recall'])\n",
    "best_f1 = max(model_results.items(), key=lambda x: x[1]['f1'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ü•á BEST PERFORMING MODELS BY METRIC\")\n",
    "print(\"=\"*100)\n",
    "print(f\"üéØ Best Accuracy:  {best_accuracy[0]} ({best_accuracy[1]['accuracy']:.4f})\")\n",
    "print(f\"üéØ Best Precision: {best_precision[0]} ({best_precision[1]['precision']:.4f})\")\n",
    "print(f\"üéØ Best Recall:    {best_recall[0]} ({best_recall[1]['recall']:.4f})\")\n",
    "print(f\"üéØ Best F1-Score:  {best_f1[0]} ({best_f1[1]['f1']:.4f})\")\n",
    "\n",
    "print(\"\\n‚úÖ All models trained and saved successfully!\")\n",
    "print(\"üìÅ Model files saved in: ../backend/models/\")\n",
    "print(\"üìä Comprehensive feature extraction completed with enhanced biochemical properties\")\n",
    "print(f\"üî¨ Total features per sequence: {features_scaled.shape[1]}\")\n",
    "print(f\"üìà Dataset size: {len(sequences):,} sequences\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
