{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_therapeutic_categories(data_path):\n",
    "    therapeutic_path = os.path.join(data_path, \"raw\", \"therapeutic\")\n",
    "    sequences, categories = [], []\n",
    "    \n",
    "    category_mapping = {\n",
    "        'anticancer': 0,\n",
    "        'antimicrobial': 1,\n",
    "        'antiviral': 2,\n",
    "        'immunomodulatory': 3,\n",
    "        'neuropeptide': 4\n",
    "    }\n",
    "    \n",
    "    for filename in os.listdir(therapeutic_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            category_name = filename.replace('.csv', '').lower()\n",
    "            if any(cat in category_name for cat in category_mapping.keys()):\n",
    "                for cat_key, cat_value in category_mapping.items():\n",
    "                    if cat_key in category_name:\n",
    "                        category = cat_value\n",
    "                        break\n",
    "            else:\n",
    "                category = len(category_mapping)\n",
    "            \n",
    "            df = pd.read_csv(os.path.join(therapeutic_path, filename))\n",
    "            sequences.extend(df.iloc[:, 0].tolist())\n",
    "            categories.extend([category] * len(df))\n",
    "    \n",
    "    return sequences, categories, list(category_mapping.keys()) + ['other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiclass_model(input_dim, num_classes):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    dense1 = Dense(512, activation='relu')(input_layer)\n",
    "    dropout1 = Dropout(0.4)(dense1)\n",
    "    \n",
    "    dense2 = Dense(256, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(0.3)(dense2)\n",
    "    \n",
    "    dense3 = Dense(128, activation='relu')(dropout2)\n",
    "    dropout3 = Dropout(0.2)(dense3)\n",
    "    \n",
    "    dense4 = Dense(64, activation='relu')(dropout3)\n",
    "    dropout4 = Dropout(0.1)(dense4)\n",
    "    \n",
    "    output = Dense(num_classes, activation='softmax')(dropout4)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data\"\n",
    "sequences, categories, category_names = load_therapeutic_categories(data_path)\n",
    "\n",
    "print(f\"Total therapeutic sequences: {len(sequences)}\")\n",
    "print(f\"Number of categories: {len(category_names)}\")\n",
    "print(f\"Categories: {category_names}\")\n",
    "\n",
    "category_counts = np.bincount(categories)\n",
    "for i, (name, count) in enumerate(zip(category_names, category_counts)):\n",
    "    print(f\"{name}: {count} sequences\")\n",
    "\n",
    "if os.path.exists(\"../data/processed/protbert_embeddings.npy\"):\n",
    "    print(\"Loading cached ProtBERT embeddings...\")\n",
    "    features = np.load(\"../data/processed/protbert_embeddings.npy\")\n",
    "    features = features[:len(sequences)]\n",
    "else:\n",
    "    print(\"ProtBERT embeddings not found. Please run 02_cnn_lstm_protbert.ipynb first.\")\n",
    "    features = np.random.randn(len(sequences), 1024)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "with open(\"../backend/models/multiclass_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_scaled, categories, test_size=0.2, random_state=42, stratify=categories\n",
    ")\n",
    "\n",
    "num_classes = len(category_names)\n",
    "model = create_multiclass_model(features_scaled.shape[1], num_classes)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "    ModelCheckpoint('../backend/models/multiclass_classifier.h5', save_best_only=True, monitor='val_loss')\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes, target_names=category_names))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=category_names, yticklabels=category_names)\n",
    "plt.title('Confusion Matrix - Therapeutic Category Classification')\n",
    "plt.ylabel('True Category')\n",
    "plt.xlabel('Predicted Category')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Divide & Conquer: Parallel Feature Extraction\n",
    "def extract_protbert_features(sequences):\n",
    "    def process_sequence(seq):\n",
    "        seq_hash = hash_sequence(seq)\n",
    "        if seq_hash in embedding_cache:\n",
    "            return embedding_cache[seq_hash]\n",
    "\n",
    "        seq = ' '.join(list(seq))  \n",
    "        encoded = protbert_tokenizer.batch_encode_plus(\n",
    "            [seq], padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = protbert_model(**encoded).last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n",
    "\n",
    "        embedding_cache[seq_hash] = output  \n",
    "        return output\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        embeddings = list(tqdm(executor.map(process_sequence, sequences), total=len(sequences), desc=\"Extracting ProtBERT Features\"))\n",
    "\n",
    "    return np.array(embeddings, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Load & Preprocess Data\n",
    "base_folder = r\"C:\\\\Users\\\\SURYA HA\\\\OneDrive\\\\Documents\\\\Prediction of Therapeutic Peptide using Deep Learning and DAA\\\\data\\\\Therapeutic Category Classification\"\n",
    "sequences, labels = load_data(base_folder)\n",
    "\n",
    "# Check if we have data\n",
    "if len(sequences) == 0:\n",
    "    raise ValueError(\"No sequences were loaded. Please check your data files.\")\n",
    "\n",
    "print(f\"Total sequences loaded: {len(sequences)}\")\n",
    "print(f\"Unique categories: {len(set(labels))}\")\n",
    "\n",
    "# Extract features\n",
    "protbert_features = extract_protbert_features(sequences)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(protbert_features)\n",
    "joblib.dump(scaler, \"scaler 2.pkl\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Feature Selection using PCA\n",
    "pca = PCA(n_components=min(50, len(sequences) - 1))  # Ensure n_components is valid\n",
    "X_pca = pca.fit_transform(X)\n",
    "joblib.dump(pca, \"pca_model.pkl\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ CNN-LSTM Model (Efficient)\n",
    "num_classes = len(set(labels))\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = Sequential([\n",
    "        Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(64),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "    \n",
    "    # Add class weights to handle imbalanced data\n",
    "    class_weights = {}\n",
    "    class_counts = np.bincount(y_train)\n",
    "    total = len(y_train)\n",
    "    for i in range(len(class_counts)):\n",
    "        class_weights[i] = total / (len(class_counts) * class_counts[i])\n",
    "    \n",
    "    # Train with class weights\n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=40, \n",
    "        batch_size=32, \n",
    "        validation_data=(X_test, y_test),\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "\n",
    "model.save(\"model 2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Accuracy Evaluation\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate classification report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Modified Confidence Adjustment with 0.3 threshold\n",
    "def confidence_adjustment(prediction):\n",
    "    max_prob = np.max(prediction)\n",
    "    if max_prob < 0.3:  # Changed threshold from 0.7 to 0.3\n",
    "        return \"Unknown Category\"\n",
    "    return label_encoder.inverse_transform([np.argmax(prediction)])[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
